{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "EE-311\n",
                "======\n",
                "\n",
                "Lab 5: Dimensionality Reduction\n",
                "----------------------------------------\n",
                "\n",
                "created by Zahra Farsijani and Fran√ßois Marelli on 25.03.2020"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Import libraries\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sklearn\n",
                "from sklearn.datasets import load_iris, load_wine, make_circles\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import train_test_split\n",
                "import sklearn.decomposition\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from warnings import filterwarnings\n",
                "filterwarnings('ignore', category=sklearn.exceptions.ConvergenceWarning)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Dimensionality Reduction\n",
                "\n",
                "Many Machine Learning problems involve thousands or even millions of features for\n",
                "each training instance. Not only does this make training extremely slow, it can also\n",
                "make it much harder to find a good solution, and to visualize the data. This problem is often\n",
                "referred to as the curse of dimensionality. \n",
                "\n",
                "In this lab we will explore different types of methods to reduce the dimenionality of our data:\n",
                "\n",
                "1. Filter Methods\n",
                "2. Wrapper Methods\n",
                "3. Feature extraction\n",
                "\n",
                "There are two main approaches for feature selection:\n",
                "\n",
                "- wrapper methods, in which the features are selected using knowledge of the model\n",
                "- filter methods, in which the selection of features is independent of model used\n",
                "\n",
                "## 1.1 Filter Methods\n",
                "\n",
                "A simple approach for feature selection is the filter method, in which we the features of the dataset are ranked according to a scoring criterion.\n",
                "\n",
                "Selecting only the highest scoring features reduces the dimensionality of the dataset.\n",
                "\n",
                "We will test this method on the iris dataset with all features, for binary classification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_X, data_y = load_iris(return_X_y=True)\n",
                "\n",
                "mask = (data_y == 0) | (data_y == 1)\n",
                "\n",
                "data_X = data_X[mask]\n",
                "data_y = data_y[mask]\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.15, random_state=0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Since we have four features, we cannot effectively plot the data. \n",
                "\n",
                "Instead, we try to get more insights into the correlation between various features by plotting 2D graphs for each possible selection of 2 features in the dataset.\n",
                "\n",
                "What do you learn about the data? Can you see which features matter for classification and which ones don't?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = sns.load_dataset(\"iris\")\n",
                "df = df[df.species != 'virginica']\n",
                "sns.pairplot(df, hue=\"species\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pearson's correlation coefficient filter\n",
                "\n",
                "A simple criterion to measure the information contained in the features is to compute the correlation between the features and the labels.\n",
                "\n",
                "Compute this criterion for all the features in the dataset, then select the two most informative features and show the resulting dataset in a scatterplot.\n",
                "\n",
                "According to the previous plot, which combination of features do you expect to be selected?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Your turn to code!\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.2 Wrapper Methods\n",
                "\n",
                "Wrapper methods select the best features by iteratively training the model and adding or removing features on the go.\n",
                "\n",
                "We will investigate the two basic versions of wrapper methods: Forward search and Backward search.\n",
                "\n",
                "### Forward search\n",
                "\n",
                "In this method, we start with having no feature in the model. In each iteration, we add the feature which best improves our model performance till the addition of a new feature does not bring any improvement anymore.\n",
                "\n",
                "Write a code that implements forward feature selection on a given model and dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def forward_search(X_train, y_train, X_test, y_test, model):\n",
                "    \"\"\"\n",
                "    Implement forward search feature selection.\n",
                "\n",
                "    Parameters\n",
                "    ----------    \n",
                "    X_train, X_test : numpy array of shape [n_samples, n_features], datasets\n",
                "        \n",
                "    y_train, y_test : numpy array of shape [n_train_samples, ], labels (binary)\n",
                "      \n",
                "    model : a sklearn model implementing the .fit() and .score() functions (e.g. LogisticRegression())\n",
                "    \n",
                "    Returns\n",
                "    -------\n",
                "    numpy array of shape [n_features_selected,]\n",
                "            Vector with the indexes of selected features, sorted by first selected\n",
                "    \"\"\"\n",
                "    \n",
                "    #######################################################\n",
                "    # Your code here!\n",
                "    \n",
                "    \n",
                "    \n",
                "    return\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Run your forward search algorithm on the iris dataset and print which features have been selected for a basic logistic regression model.\n",
                "\n",
                "What dimensions are selected?\n",
                "\n",
                "Did you expect it?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Code here!\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### The wine dataset\n",
                "\n",
                "The iris dataset is too simple to demonstrate the use of wrapper methods.\n",
                "\n",
                "We will use a bigger dataset called [the wine dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine).\n",
                "\n",
                "How many features does this one have?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and prepare data\n",
                "data_X_wine, data_y_wine = load_wine(return_X_y=True)\n",
                "\n",
                "mask = (data_y_wine == 0) | (data_y_wine == 1)\n",
                "\n",
                "data_X_wine = data_X_wine[mask]\n",
                "data_y_wine = data_y_wine[mask]\n",
                "\n",
                "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(data_X_wine, data_y_wine, test_size=0.35, random_state=3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Use your forward search function to reduce the wine dataset using a linear regression model, and show what features have been selected.\n",
                "\n",
                "Further reduce the dimensionality by keeping only the 2 most relevant features.\n",
                "\n",
                "Then, compare the model accuracy for both selected features and ALL features for the wine dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = LogisticRegression()\n",
                "\n",
                "features = forward_search(X_train_wine, y_train_wine, X_test_wine, y_test_wine, model)\n",
                "\n",
                "\n",
                "# Here you code!\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Backward Search (bonus)\n",
                "\n",
                "In this method, as opposed to previous one, we start with using all features to train the model. \n",
                "\n",
                "In each iteration, we remove the feature which has the least effect on our model, untill removing of a new variable does not improve the performance of the model.\n",
                "\n",
                "Write a code that implements forward feature selection on a given model and dataset. It can be very similar to your forward search!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def backward_search(X_train, y_train, X_test, y_test, model):\n",
                "    \"\"\"\n",
                "    Implement backward search feature selection.\n",
                "\n",
                "    Parameters\n",
                "    ----------    \n",
                "    X_train, X_test : numpy array of shape [n_samples, n_features], datasets\n",
                "        \n",
                "    y_train, y_test : numpy array of shape [n_train_samples, ], labels (binary)\n",
                "      \n",
                "    model : a sklearn model implementing the .fit() and .score() functions (e.g. LogisticRegression())\n",
                "    \n",
                "    Returns\n",
                "    -------\n",
                "    numpy array of shape [n_features_selected,]\n",
                "            Vector with the indexes of selected features, sorted by importance of feature\n",
                "    \"\"\"\n",
                "    \n",
                "    \n",
                "    #######################################################\n",
                "    # Code here\n",
                "    \n",
                "    \n",
                "    \n",
                "    return\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Use your backward search function to reduce the wine dataset for a linear regression model.\n",
                "\n",
                "Can you reduce it further down to 2 features only? (*Spoiler: yes you can, but how?*)\n",
                "\n",
                "How does the ordering of features compare to the one obtained with the forward search?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = LogisticRegression()\n",
                "\n",
                "features = backward_search(X_train_wine, y_train_wine, X_test_wine, y_test_wine, model)\n",
                "\n",
                "\n",
                "# Here we code again!\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Feature extraction using PCA\n",
                "\n",
                "Where feature selection only tries to find the most important in the dataset, feature extraction actually combines multiple features to create new (better?) features.\n",
                "\n",
                "We will focus on a well-known feature extraction method: the Principal Component Analysis (PCA).\n",
                "\n",
                "PCA tries to define a new coordinate system such that the variance of the data is maximized along its main axes.\n",
                "\n",
                "This then allows to select only the first few axes to reduce the dimension of the space while still keeping most of the representative information contained in the data.\n",
                "\n",
                "**In this lab (and in the homework), we choose to not normalize 2D data for illustration purposes. We will later demonstrate the benefits of normalization on higher dimensionality datasets.**\n",
                "\n",
                "For demonstration purposes, we will create a toy dataset that illustrates the concepts of PCA. \n",
                "\n",
                "Wait... There are no labels!? Is this important?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_X_toy = np.random.default_rng(0).multivariate_normal([1, 0.5], [[1, 0], [0, 25]], 500)\n",
                "\n",
                "# Rotate the gaussian\n",
                "theta = np.radians(60)\n",
                "c, s = np.cos(theta), np.sin(theta)\n",
                "R = np.array(((c, -s), (s, c)))\n",
                "data_X_toy = data_X_toy.dot(R)\n",
                "data_X_toy += (10, -5)\n",
                "\n",
                "plt.scatter(data_X_toy[:,0], data_X_toy[:,1])\n",
                "plt.grid()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "PCA is computed by extracting the eigenvectors of the covariance matrix of the data. \n",
                "\n",
                "Write the code to compute the PCA and plot the principal directions on the graph using `draw_vector`. \n",
                "\n",
                "How do you interpret it? Can you tell which is the principal axis and how?\n",
                "\n",
                "You can use:\n",
                "\n",
                "`np.cov` to compute covariance\n",
                "\n",
                "`np.linalg.eig` to perform eigen decomposition of a matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def draw_vector(v0, v1, color='b', ax=None):\n",
                "    \"\"\"\n",
                "    Draw a vector on a matplotlib graph.\n",
                "\n",
                "    Parameters:\n",
                "        v0: np.array of size (2), the starting point of the vector\n",
                "        v1: np.array of size (2), the end point of the vector\n",
                "        color: str, color code for the plotting\n",
                "        ax: which axes to plot to, leave empty to use current figure\n",
                "    \"\"\"\n",
                "    \n",
                "    ax = ax or plt.gca()\n",
                "    arrowprops=dict(ec=color,\n",
                "                    arrowstyle='->',\n",
                "                    linewidth=2,\n",
                "                    shrinkA=0, shrinkB=0)\n",
                "    ax.annotate('', v1, v0, arrowprops=arrowprops, color='r')\n",
                "\n",
                "\n",
                "# Code here, you must.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A numerically more stable way to compute PCA is Singular Value Decomposition (SVD).\n",
                "\n",
                "Compute the principal components of the data using `np.linalg.svd` and check that the result is identical.\n",
                "\n",
                "Do not forget to center the data first (zero mean)! Why is it important? Try without that and see the result."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Program here\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Dimensionality reduction\n",
                "\n",
                "We can use PCA to reduce the dimension of a dataset by projecting it onto its N first principal components.\n",
                "\n",
                "Let us show it on the iris dataset. Use SVD to compute its principal components, and plot the percentage of information contained in each principal component (ratio of variance explained by each component).\n",
                "\n",
                "How many features seem relevant?\n",
                "\n",
                "You can use `plt.bar( . )` for the figure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Your code here\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can choose to keep only the two first principal components to reduce the dimensionality of the dataset.\n",
                "\n",
                "Reminder: to project the data into the reduced space, compute the matrix multiplication of the dataset matrix $X$ by the matrix $W_{d}$, defined as the matrix containing the first $d$ principal vectors (i.e., the matrix composed of the first $d$ right eigenvectors).\n",
                "\n",
                "Reduce the iris dataset to a dimension of 2 using PCA and show the result as a scatterplot."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Time to shine\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Fortunately enough, you do not have to write all this every time, as `sklearn.decomposition.PCA` is there for you!\n",
                "\n",
                "Use it to reduce the iris dataset to a dimension of 2, and show a scatter of the result."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# One more time\n",
                "\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0-final"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}