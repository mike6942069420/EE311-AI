{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "EE-311\n",
                "======\n",
                "\n",
                "Lab 4: Support Vector Machines (SVM)\n",
                "------------------------------------\n",
                "\n",
                "created by Natalie Bolón Brun and François Marelli on 11.03.2020"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Import libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.datasets import load_iris, make_circles\n",
                "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load and prepare data\n",
                "\n",
                "We will start with the same dataset as last week, using only the two first features. \n",
                "\n",
                "Since we want to explore the case of linear models, start start with checking if it is linearly separable on the scatter plot. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_X, data_y = load_iris(return_X_y=True)\n",
                "\n",
                "mask = (data_y == 0) | (data_y == 1)\n",
                "\n",
                "data_X = data_X[mask]\n",
                "data_y = data_y[mask]\n",
                "\n",
                "data_X = data_X[:,0:2]\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.15, random_state=0)\n",
                "\n",
                "\n",
                "plt.figure(figsize=(10,5))\n",
                "plt.scatter(data_X[:,0], data_X[:,1], c=data_y)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Compare SVM with logistic regression\n",
                "\n",
                "Last week we used logistic regression for classification. This week we will investigate SVM. \n",
                "\n",
                "We will start by comparing their behaviour for a SVM with hard margins.\n",
                "\n",
                "1. What should we expect on our dataset?\n",
                "2. What is the C argument in the SVC? Why do we set it so high?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# train logistic regression classifier\n",
                "classifier_lr = LogisticRegression().fit(X_train, y_train)\n",
                "\n",
                "# train SVM classifier\n",
                "classifier_svm = SVC(kernel='linear', C=10000).fit(X_train, y_train)\n",
                "\n",
                "# Compute the accuracy\n",
                "accuracy_lr = classifier_lr.score(X_test, y_test)\n",
                "accuracy_svm = classifier_svm.score(X_test, y_test)\n",
                "\n",
                "print('Classification with Logistic Regression. Accuracy: {:.2f}%'.format(accuracy_lr * 100))\n",
                "print('Classification with Support Vector Machine. Accuracy: {:.2f}%'.format(accuracy_svm * 100))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Compare decision boundaries\n",
                "\n",
                "To check the difference between the two classifiers plot the decision boundary for each one. \n",
                "\n",
                "* Which classifier seems better? Why?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "f, ax = plt.subplots(figsize=(10,5))\n",
                "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30)\n",
                "\n",
                "# get limits of the figure\n",
                "ax = plt.gca()\n",
                "xlim = ax.get_xlim()\n",
                "ylim = ax.get_ylim()\n",
                "\n",
                "x_plot = np.linspace(xlim[0], xlim[1], 30)\n",
                "\n",
                "\n",
                "##################################################################\n",
                "# Your code here\n",
                "\n",
                "y_lr = None\n",
                "\n",
                "y_svm = None\n",
                "\n",
                "##################################################################\n",
                "\n",
                "\n",
                "ax.plot(x_plot, y_lr, '--', label='Boundary_LR')\n",
                "ax.plot(x_plot, y_svm, '-', label='Boundary_SVM')\n",
                "\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Compute Margins for each classifier\n",
                "\n",
                "We have seen the decision boundaries are not equal. We can compute the margin of each classifier to evaluate numerically this difference. \n",
                "\n",
                "1. Which classifier has larger margin?\n",
                "2. Is the margin to all classes equal?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def distance(b, w, X):\n",
                "    \"\"\"\n",
                "    Compute distance between a line and a point.\n",
                "    The equation of the line given by: b + w[0] * X[:, 0] + w[1] * X[:, 1] = 0\n",
                "    \n",
                "    b: model intercept\n",
                "    w: model coefficients\n",
                "    X: data points (N, 2)\n",
                "    \n",
                "    Return: float, distance to hyperplane\n",
                "    \"\"\"\n",
                "    \n",
                "    #######################################################\n",
                "    # Your code here\n",
                "    \n",
                "    \n",
                "    \n",
                "    #######################################################\n",
                "\n",
                "\n",
                "# first split the data by class\n",
                "x_0 = data_X[data_y == 0]\n",
                "x_1 = data_X[data_y == 1]\n",
                "\n",
                "\n",
                "############################################################\n",
                "# Code here to compute the margins for SVM and LR\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualize margins and support vectors\n",
                "\n",
                "This function plots the decision hyperplane, the margin hyperplanes, and highlights the support vectors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_margins(X_train, y_train, classifier_svm):\n",
                "\n",
                "    f, ax = plt.subplots(figsize=(10,5))\n",
                "    if X_train.shape[1] == 2:\n",
                "        plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
                "    else:\n",
                "        plt.scatter(X_train,X_train, c=y_train)\n",
                "\n",
                "    # plot the decision function\n",
                "    ax = plt.gca()\n",
                "    xlim = ax.get_xlim()\n",
                "    ylim = ax.get_ylim()\n",
                "\n",
                "    # create grid to evaluate model\n",
                "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
                "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
                "    YY, XX = np.meshgrid(yy, xx)\n",
                "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
                "    Z = classifier_svm.decision_function(xy).reshape(XX.shape)\n",
                "\n",
                "    # plot decision boundary and margins\n",
                "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
                "               linestyles=['--', '-', '--'])\n",
                "    # plot support vectors\n",
                "    ax.scatter(classifier_svm.support_vectors_[:, 0], classifier_svm.support_vectors_[:, 1], s=100,\n",
                "               linewidth=1, facecolors='none', edgecolors='k')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let us use it to see the properties of our SVM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "visualize_margins(X_train, y_train, classifier_svm)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Soft-margin SVM\n",
                "\n",
                "Manually generate a linearly separable dataset, then \n",
                "add noise with increasing amplitude until the dataset is not perfectly \n",
                "separable anymore.\n",
                "\n",
                "Let's explore how we can use SVM with soft-margins and how the penalty term affects our classifier.\n",
                "\n",
                "Modify the variance (sigma) of each class to see how the data goes from linearly separable to not separable anymore. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "num_points = 100\n",
                "\n",
                "\n",
                "plt.figure(figsize=(10, 7))\n",
                "\n",
                "##################################################################\n",
                "# Increase these variances until the datasets become non separable\n",
                "\n",
                "rg = np.random.default_rng(0)\n",
                "sigma_0 = 0.1\n",
                "sigma_1 = 0.1\n",
                "\n",
                "\n",
                "# We define the datasets by their mean and covariance\n",
                "cov_0 = [[sigma_0, 0.05], [0.05, 0.8]]\n",
                "cov_1 = [[sigma_1, 0.05], [0.05, 0.3]]\n",
                "mu_0 = [1, 0.5]\n",
                "mu_1 = [-1, -0.5]\n",
                "\n",
                "X_class0 = rg.multivariate_normal(mu_0, cov_0, num_points)\n",
                "X_class1 = rg.multivariate_normal(mu_1, cov_1, num_points)\n",
                "\n",
                "plt.scatter(X_class0[:,0],X_class0[:,1],label='class 0')\n",
                "plt.scatter(X_class1[:,0],X_class1[:,1],label='class 1')\n",
                "plt.grid()\n",
                "plt.legend()\n",
                "    \n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For the non-separable case, train a classifier varying the penalty term.\n",
                "\n",
                "Show the evolution of the decision hyperplane and the margins using the function *visualize_margins*.\n",
                "\n",
                "To modify the penalty term you can check the documentation of the function:\n",
                "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
                "    \n",
                "1. What do you notice about the margins?\n",
                "2. How do you explain this behaviour?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preparing the dataset for training\n",
                "X = np.concatenate([X_class0, X_class1])\n",
                "y = np.concatenate([np.zeros(num_points), np.ones(num_points)])\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
                "\n",
                "\n",
                "#######################################################\n",
                "# Time to code!\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Non-linearly separable data\n",
                "\n",
                "In this part we will use a n non-linearly separable dataset and very¡ify the poor performance of SVM. \n",
                "\n",
                "Nevertheless, the data may be linearly separable in an transformed space. Let's see how can we solve it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X, y = make_circles(n_samples=400, noise=0.1, factor=0.3, random_state=0)\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
                "\n",
                "plt.figure(figsize=(7, 7))\n",
                "plt.scatter(X[:,0],X[:,1],c=y )\n",
                "plt.axis('square')\n",
                "plt.grid()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Given the following data, answer the following:\n",
                "\n",
                "1. Is it separable?\n",
                "2. Is it linearly separable?\n",
                "3. Is a linear SVM going to perform well?\n",
                "\n",
                "Train a SVM linear classifier. \n",
                "\n",
                "Does it work? What's the accuracy?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "clr = SVC(kernel='linear')\n",
                "\n",
                "\n",
                "#######################################################\n",
                "# Your code here\n",
                "\n",
                "\n",
                "\n",
                "#######################################################\n",
                "\n",
                "\n",
                "visualize_margins(X, y, clr)\n",
                "plt.axis('square')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Apply a transformation to X_train so that the data becomes linearly separable, and plot it.\n",
                "\n",
                "Then train a SVM classifier with linear kernel, and plot its boundary and margins.\n",
                "\n",
                "What's the accuracy on the transformed dataset?\n",
                "\n",
                "*Hint: think about the equations of a circle for the transformation*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "clr = SVC(kernel='linear', C=1e6)\n",
                "\n",
                "\n",
                "################################################################################################\n",
                "# Your code here: transform the dataset, plot it then train a SVM, plot its accuracy and margins\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## The kernel trick\n",
                "\n",
                "Instead of transforming the state, we can use an appropriate kernel to train the SVM.\n",
                "\n",
                "The kernel redefines the notion of distance with respect to the SVM boundary. It is equivalent to transforming the space and then computing distances to a hyperplane, but requires less complex computations.\n",
                "\n",
                "A very commonly used kernel is the RBF (radial basis function). It corresponds to a transformation to a space of infinite dimension.\n",
                "\n",
                "Train a SVM with RBF kernel on the non-transformed data and check its accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "clr = SVC(kernel='rbf', C=1e6)\n",
                "\n",
                "\n",
                "#########################################\n",
                "# Train the model and print the accuracy\n",
                "\n",
                "\n",
                "\n",
                "#########################################\n",
                "\n",
                "\n",
                "visualize_margins(X, y, clr)\n",
                "plt.axis('square')\n",
                "plt.grid()\n",
                "plt.show()"
            ]
        },
        {
            "source": [
                "## Bonus: outlier detection\n",
                "\n",
                "These plots show some of the datasets used to test outlier detection. The outliers are highlighted in purple.\n",
                "\n",
                "An outlier is a data point with an extreme value that diverges from the rest of the dataset. It can sometimes be due to experimental errors, and can then decrease the performance of models as they try to model extreme values that should not be there in practice. It can therefore be useful to be able to detect outliers in a dataset to deal with them appropriately.\n",
                "\n",
                "It must be noted that outliers do not always represent errors, sometimes they can be due to very rare but genuine events. Therefore discarding outliers must only be done after a precise analysis and a serious reflexion.\n",
                "\n",
                "Implement a function to detect the outlier automatically. Think about what characterises an outlier!"
            ],
            "cell_type": "markdown",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "outliers = np.load('outliers.npz')\n",
                "indexes = outliers['index']\n",
                "\n",
                "\n",
                "def detect_outlier(X):\n",
                "    '''\n",
                "    Detect the outlier in dataset X and return its position (0-based indexing)\n",
                "    \n",
                "    X: array (N, M) with N being the number of points and M the number of features\n",
                "    \n",
                "    Return: int, the position index of the outlier in the dataset\n",
                "    '''\n",
                "    \n",
                "    ##############################################\n",
                "    # Code here\n",
                "\n",
                "\n",
                "    return\n",
                "\n",
                "    #############################################\n",
                "\n",
                "\n",
                "outlier = outliers['0']\n",
                "idx = indexes[0]\n",
                "color = np.ones(outlier.shape[0])\n",
                "color[idx] = 0\n",
                "plt.scatter(outlier[:, 0], outlier[:, 1], c=color)\n",
                "plt.axis('equal')\n",
                "plt.show()\n",
                "\n",
                "\n",
                "outlier = outliers['1']\n",
                "idx = detect_outlier(outlier)\n",
                "color = np.ones(outlier.shape[0])\n",
                "if idx is not None:\n",
                "    color[idx] = 0\n",
                "plt.scatter(outlier[:, 0], outlier[:, 1], c=color)\n",
                "plt.axis('equal')\n",
                "plt.show()\n",
                "\n",
                "\n",
                "outlier = outliers['2']\n",
                "fig = plt.figure(figsize=(10, 7))\n",
                "ax = fig.add_subplot(111, projection='3d')\n",
                "\n",
                "color = np.ones(outlier.shape[0])\n",
                "idx = detect_outlier(outlier)\n",
                "if idx is not None:\n",
                "    color[idx] = 0\n",
                "ax.scatter(outlier[:,0], outlier[:,1], outlier[:,2], c=color)\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.2-final"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}